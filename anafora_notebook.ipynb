{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ανάκτηση Πληροφορίας\n",
    "\n",
    "Σουκαράς Σωτήριος ice21390206\n",
    "Θεοφάνης Κουνιάκης ice21390103\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 1: Συλλογή δεδομένων\n",
    "\n",
    "Για τη συλλογή δεδομένων έχει καταστευάσετει ένα αναδρομικό προγράμμα Data Scrape για Wikipedia. Το πρόγραμμα εξερευνεί αναδρομικά όλους τους συνδέσμους που περιέχονται μέσα σε μια σελίδα της Wikipedia μέχρι ένα συγκεκριμένο βάθος. Τα αποτελέσματα αποθηκεύονται στο αρχείο \"wiki_scrape.json\" σε μορφή οπού κάθε json αντικείμενο περιέχει το Link του ιστότοπου και τις παραγρφαφούς του ιστότοπου σε μια λίστα.\n",
    "\n",
    "**To ID \"mw-content-text\"** <br>\n",
    "Στις σελίδες της Wikipedia όλο το ουσιαστικό περιεχόμενο περιέχεται μέσα στο id \"mw-content-text\". Επομένος δεν εξερευνούνται links που οδηγούν στην αρχική σελιδα της Wikipedia για παράδειγμα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching https://en.wikipedia.org/wiki/World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/WWII_(disambiguation)...\n",
      "Searching https://en.wikipedia.org/wiki/The_Second_World_War_(disambiguation)...\n",
      "Searching https://en.wikipedia.org/wiki/World_War_II_(disambiguation)...\n",
      "Searching https://en.wikipedia.org/wiki/Junkers_Ju_87...\n",
      "Searching https://en.wikipedia.org/wiki/Eastern_Front_(World_War_II)...\n",
      "Searching https://en.wikipedia.org/wiki/Matilda_II...\n",
      "Searching https://en.wikipedia.org/wiki/North_African_campaign...\n",
      "Searching https://en.wikipedia.org/wiki/Atomic_bombings_of_Hiroshima_and_Nagasaki...\n",
      "Searching https://en.wikipedia.org/wiki/Battle_of_Stalingrad...\n",
      "Searching https://en.wikipedia.org/wiki/Raising_a_Flag_over_the_Reichstag...\n",
      "Searching https://en.wikipedia.org/wiki/Reichstag_building...\n",
      "Searching https://en.wikipedia.org/wiki/Battle_of_Berlin...\n",
      "Searching https://en.wikipedia.org/wiki/Invasion_of_Lingayen_Gulf...\n",
      "Searching https://en.wikipedia.org/wiki/Japanese_occupation_of_the_Philippines...\n",
      "Searching https://en.wikipedia.org/wiki/Theater_(warfare)...\n",
      "Searching https://en.wikipedia.org/wiki/European_theatre_of_World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/Pacific_War...\n",
      "Searching https://en.wikipedia.org/wiki/Battle_of_the_Atlantic...\n",
      "Searching https://en.wikipedia.org/wiki/Indian_Ocean_in_World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/South-East_Asian_theatre_of_World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/Second_Sino-Japanese_War...\n",
      "Searching https://en.wikipedia.org/wiki/Air_raids_on_Japan...\n",
      "Searching https://en.wikipedia.org/wiki/Mediterranean_and_Middle_East_theatre_of_World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/Mediterranean_and_Middle_East_theatre_of_World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/North_African_campaign...\n",
      "Searching https://en.wikipedia.org/wiki/East_African_campaign_(World_War_II)...\n",
      "Searching https://en.wikipedia.org/wiki/Battle_of_Gabon...\n",
      "Searching https://en.wikipedia.org/wiki/Attacks_on_Australia_during_World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/Battle_of_the_Caribbean...\n",
      "Searching https://en.wikipedia.org/wiki/American_Theater_(World_War_II)...\n",
      "Searching https://en.wikipedia.org/wiki/Allies_of_World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/Aftermath_of_World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/World_War_II_by_country...\n",
      "Searching https://en.wikipedia.org/wiki/Allies_of_World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/Axis_powers...\n",
      "Searching https://en.wikipedia.org/wiki/Allied_leaders_of_World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/Soviet_Union...\n",
      "Searching https://en.wikipedia.org/wiki/Joseph_Stalin...\n",
      "Searching https://en.wikipedia.org/wiki/United_States...\n",
      "Searching https://en.wikipedia.org/wiki/Franklin_D._Roosevelt...\n",
      "Searching https://en.wikipedia.org/wiki/United_Kingdom...\n",
      "Searching https://en.wikipedia.org/wiki/Winston_Churchill...\n",
      "Searching https://en.wikipedia.org/wiki/Republic_of_China_(1912%E2%80%931949)...\n",
      "Searching https://en.wikipedia.org/wiki/Chiang_Kai-shek...\n",
      "Searching https://en.wikipedia.org/wiki/Axis_leaders_of_World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/Nazi_Germany...\n",
      "Searching https://en.wikipedia.org/wiki/Adolf_Hitler...\n",
      "Searching https://en.wikipedia.org/wiki/Empire_of_Japan...\n",
      "Searching https://en.wikipedia.org/wiki/Hirohito...\n",
      "Searching https://en.wikipedia.org/wiki/Fascist_Italy...\n",
      "Searching https://en.wikipedia.org/wiki/Benito_Mussolini...\n",
      "Searching https://en.wikipedia.org/wiki/World_War_II_casualties...\n",
      "Searching https://en.wikipedia.org/wiki/World_War_II_casualties...\n",
      "Searching https://en.wikipedia.org/wiki/Template:Campaignbox_World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/Template_talk:Campaignbox_World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/Special:EditPage/Template:Campaignbox_World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/European_theatre_of_World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/Invasion_of_Poland...\n",
      "Searching https://en.wikipedia.org/wiki/Polish_resistance_movement_in_World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/Phoney_War...\n",
      "Searching https://en.wikipedia.org/wiki/Finland_in_World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/Winter_War...\n",
      "Searching https://en.wikipedia.org/wiki/Continuation_War...\n",
      "Searching https://en.wikipedia.org/wiki/Lapland_War...\n",
      "Searching https://en.wikipedia.org/wiki/Operation_Weser%C3%BCbung...\n",
      "Searching https://en.wikipedia.org/wiki/Western_Front_(World_War_II)...\n",
      "Searching https://en.wikipedia.org/wiki/Battle_of_France...\n",
      "Searching https://en.wikipedia.org/wiki/French_Resistance...\n",
      "Searching https://en.wikipedia.org/wiki/Western_Front_(World_War_II)#1944–1945:_The_Second_Front...\n",
      "Searching https://en.wikipedia.org/wiki/Italian_invasion_of_France...\n",
      "Searching https://en.wikipedia.org/wiki/Second_Battle_of_the_Alps...\n",
      "Searching https://en.wikipedia.org/wiki/Battle_of_Britain...\n",
      "Searching https://en.wikipedia.org/wiki/Balkans_campaign_(World_War_II)...\n",
      "Searching https://en.wikipedia.org/wiki/Yugoslav_Partisans...\n",
      "Searching https://en.wikipedia.org/wiki/Greek_resistance...\n",
      "Searching https://en.wikipedia.org/wiki/Eastern_Front_(World_War_II)...\n",
      "Searching https://en.wikipedia.org/wiki/1940%E2%80%931944_insurgency_in_Chechnya...\n",
      "Unable to parse link: https://en.wikipedia.org/wiki/World_War_II\n",
      "Data saved to JSON file: wiki_scrape.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_wikipedia(URL, depth_limit, depth = 1):\n",
    "    parsed_paragraphs = {}\n",
    "    print(\"Searching \" + URL + \"...\")\n",
    "    \n",
    "    try:\n",
    "        wiki_responce = requests.get(URL)\n",
    "        wiki_responce.raise_for_status()        # Throw if error was encountered in the request\n",
    "\n",
    "        # Parse the responce with BeautifulSoup\n",
    "        soup_responce = BeautifulSoup(wiki_responce.text, 'html.parser')\n",
    "        soup_paragraphs = soup_responce.find_all('p')\n",
    "\n",
    "        # Remove html tags and append them to the return values if they have text\n",
    "        parsed_paragraphs[URL] = [p.text.strip() for p in soup_paragraphs if p.text.strip() != \"\"]\n",
    "\n",
    "        # If the maxt depth of the search has been reached exit the recursion\n",
    "        if depth >= depth_limit:\n",
    "            return parsed_paragraphs\n",
    "        \n",
    "        # Find the main content of the wiki article if exists\n",
    "        body = soup_responce.find(id=\"mw-content-text\")\n",
    "        if not body:\n",
    "            return parsed_paragraphs\n",
    "        \n",
    "        for link in body.find_all('a'):\n",
    "            # If the href tag in not present or it doesn't point to an other wiki side skip it\n",
    "            if not ('href' in link.attrs) or link['href'].find(\"/wiki/\") == -1 or link['href'].find(\"File:\") != -1:\n",
    "                continue\n",
    "\n",
    "            # Search the next wiki link\n",
    "            new_paragraphs = fetch_wikipedia(\"https://en.wikipedia.org\" + link['href'], depth_limit, depth + 1)\n",
    "            # Dont spam the wiki database\n",
    "            time.sleep(1)       \n",
    "\n",
    "            # Return value is valid\n",
    "            if not new_paragraphs:\n",
    "                continue\n",
    "            \n",
    "            # Append the return values to the dictionary \n",
    "            parsed_paragraphs.update(new_paragraphs)\n",
    "\n",
    "        return parsed_paragraphs\n",
    "    except:\n",
    "        print(\"Unable to parse link: \" + URL)\n",
    "        return parsed_paragraphs\n",
    "\n",
    "\n",
    "#Fetch info for link with max recusive search of 2\n",
    "results = fetch_wikipedia(\"https://en.wikipedia.org/wiki/World_War_II\", 2)\n",
    "\n",
    "filename = \"wiki_scrape.json\"\n",
    "\n",
    "# Convert to json object\n",
    "json_object = [\n",
    "    {\n",
    "        \"website_url\": website,\n",
    "        \"content\": data_list, \n",
    "    }\n",
    "    for website, data_list in results.items()\n",
    "]\n",
    "\n",
    "# Save as JSON file\n",
    "try:\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(json_object, file, indent=4)\n",
    "    print(f\"Data saved to JSON file: {filename}\")\n",
    "except IOError as e:\n",
    "    print(f\"Error saving to JSON file: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Setup\n",
    "Για τη σωστή λειουργεία των υπόλοιπων προγραμμάτων απαιτείται η βιβλιοθήκη NLTK. Ο παρακάτων κώδικας ελένχει και κατεβά\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kouniakis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kouniakis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kouniakis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kouniakis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kouniakis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kouniakis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download()\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 2. Προεπεξεργασία κειμένου (Text Processing):\n",
    "Για την προεπεξεργασία του κειμένου, δημιουργήθηκε πρόγραμμα που να παίρνει το .json αρχείο του βήματος 1 και:\n",
    "1) χωρίζει τα κείμενα σε λέξεις (tokenization), \n",
    "2) αφαιρεί τα stop words (πχ 'is', 'the', 'or'),\n",
    "3) και μετατρέπει κάθε λέξη σε λεξικογραφική μορφή (lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to JSON file: parsed_scrape.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re       #REGEX\n",
    "\n",
    "import nltk.stem\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def clear_special_char(text: str) -> str:\n",
    "    brackets_regex = r\"\\[[^\\]]*\\]\"\n",
    "    alpharethmetic_regex = r\"[^a-zA-Z0-9\\s]\"\n",
    "\n",
    "    # Remove the references like [55] or [a]\n",
    "    parsed_text = re.sub(brackets_regex, \"\", text)\n",
    "\n",
    "    # The '-' many times is used as seperator to seperate the words with a ' '\n",
    "    parsed_text = re.sub('-', \" \", parsed_text)\n",
    "    # Remove any non alapharithmetic char\n",
    "    parsed_text = re.sub(alpharethmetic_regex, \"\", parsed_text)\n",
    "\n",
    "    parsed_text.strip()\n",
    "\n",
    "    return parsed_text\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    # Lemmatizer and stop word objects for english\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    # Tokenize the paragraph\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove all the words inside the stop word container\n",
    "    non_stopwords_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Lematize the remain words\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in non_stopwords_tokens]\n",
    "\n",
    "    # Join all the lemmatized words into a string\n",
    "    final_text = \" \".join(lemmatized)\n",
    "    return final_text\n",
    "\n",
    "# Open the save file\n",
    "filepath = \"wiki_scrape.json\"\n",
    "with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Parse all the text in the scrape file\n",
    "parsed_data = {}\n",
    "for index, site_data in enumerate(data):\n",
    "    parsed_content = []\n",
    "    link = site_data[\"website_url\"]\n",
    "    for p_index, paragraph in enumerate(site_data[\"content\"]):\n",
    "        # For each paragraph run clean and preprocess\n",
    "        clear_paragraph = clear_special_char(paragraph)\n",
    "        parsed_paragraph = preprocess_text(clear_paragraph)\n",
    "        parsed_content.append(parsed_paragraph)\n",
    "    \n",
    "    parsed_data[link] = parsed_content\n",
    "\n",
    "# Convert to json object\n",
    "json_object = [\n",
    "    {\n",
    "        \"website_url\": website,\n",
    "        \"content\": data_list, \n",
    "    }\n",
    "    for website, data_list in parsed_data.items()\n",
    "]\n",
    "\n",
    "# Save as JSON file\n",
    "filename = \"parsed_scrape.json\"\n",
    "try:\n",
    "    with open(filename, \"w\") as file:\n",
    "        json.dump(json_object, file, indent=4)\n",
    "    print(f\"Data saved to JSON file: {filename}\")\n",
    "except IOError as e:\n",
    "    print(f\"Error saving to JSON file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 3. Ευρετήριο (Indexing):\n",
    "Το πρόγραμμα indexing δέχεται το προεπεξεργασμένο κείμενο και δημιουργεί το inverted index. To inverted index είναι μια δομή που κάθε λέξη συνδεέται με μια λίστα στην οποία είναι όλα τα έγγραφα που εμφανίζεται η λέξη. Τα γράμματα απο τις λέξεις μετατρέπονται σε μικρά ώστε τα δεδομένα να είναι ομοιόμορφα, άρα και πίο ευκολά να προσπελαθούν. <br>\n",
    "Τέλος το inverted index αποθηκεύεται στο αρχείο \"parsed_scrape.json\". <br><br>\n",
    "\n",
    "**Μορφή Inverted Index:** <br>\n",
    "`word: [ document_link1, document_link2, document_link3 ]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index saved to: inverted_index.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def create_inverted_index(data):\n",
    "    inverted_index = defaultdict(list)\n",
    "\n",
    "    # For all the enties in the file\n",
    "    for index, site_data in enumerate(data):\n",
    "        link = site_data[\"website_url\"]\n",
    "\n",
    "        # For each paragraph of the entry\n",
    "        for p_index, paragraph in enumerate(site_data[\"content\"]):\n",
    "            # Remove any caps\n",
    "            paragraph = paragraph.lower();\n",
    "\n",
    "            # Take only the unique words inside the paragraph\n",
    "            # Split them using the ' '\n",
    "            words = set(paragraph.split()) \n",
    "            \n",
    "            # For every unique word in paragraph\n",
    "            for word in words:\n",
    "                # If the website link is not present in the on the word entry append it  \n",
    "                if link not in inverted_index[word]:\n",
    "                    inverted_index[word].append(link)\n",
    "\n",
    "    return inverted_index\n",
    "\n",
    "# Open the save file\n",
    "filepath = \"parsed_scrape.json\"\n",
    "with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "inverted_index = create_inverted_index(data)\n",
    "\n",
    "# Convert to dict\n",
    "inverted_index = dict(inverted_index)\n",
    "\n",
    "# Save to JSON\n",
    "index_filename = \"inverted_index.json\"\n",
    "try:\n",
    "    with open(index_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(inverted_index, file, ensure_ascii=False, indent=4)\n",
    "    print(f\"Inverted index saved to: {index_filename}\")\n",
    "except IOError as e:\n",
    "    print(f\"Error saving inverted index: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 4. Μηχανή αναζήτησης (Search Engine):\n",
    "#### α) Επεξεργασία ερωτήματος (Query Processing):\n",
    "Ο επεξεργαστής ερωτήματος δέχεται ένα απλό ερώτημα bool και αφού εντοπίσει τα έγγραφα από τις λέξεις που ζητήθηκαν, δημιουργεί την απάντηση λαμβάνοντας υπόψη τους λογικούς τελεστές ανάμεσα στις λέξεις. Οι λογικοι τελεστές που υλοποιούνται είναι οι AND, OR, NOT και η προκαθορισμένη πράξη (Αν δεν δoθεί λογικος τελεστής) είναι η OR. <br>\n",
    "Από σύνολο των Stopwords αφαιρούνται οι λογικοί τελεστές AND, OR, NOT καθώς δεν θέλουμε να σβηστούν κατα τη διάρκεια της προεπεξεργασίας του ερωτήματος.\n",
    "\n",
    "Στο παράδειγμα τρέξαμε το Query: *Pacific and not Asia*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Atomic_bombings_of_Hiroshima_and_Nagasaki\n",
      "https://en.wikipedia.org/wiki/Winston_Churchill\n",
      "https://en.wikipedia.org/wiki/Invasion_of_Lingayen_Gulf\n",
      "https://en.wikipedia.org/wiki/Chiang_Kai-shek\n",
      "https://en.wikipedia.org/wiki/Adolf_Hitler\n",
      "https://en.wikipedia.org/wiki/Theater_(warfare)\n",
      "https://en.wikipedia.org/wiki/Matilda_II\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# NLTK imports\n",
    "import nltk.stem\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def request_query(query: str, index: dict) -> set:\n",
    "    logic_operators = {\"and\", \"or\", \"not\"};\n",
    "\n",
    "    # Init nltk objects\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english')) - logic_operators\n",
    "\n",
    "    # Tokenize query\n",
    "    tokens = word_tokenize(query.lower())\n",
    "\n",
    "    # Remove all the stop words inside the query tokens\n",
    "    non_stopwords_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Lematize the remain words\n",
    "    lemmatized_query = [lemmatizer.lemmatize(word) for word in non_stopwords_tokens];\n",
    "\n",
    "    # Remove any duplicates\n",
    "    result = set()\n",
    "\n",
    "    # Default search op is logic or\n",
    "    op = \"or\"\n",
    "\n",
    "    for token in lemmatized_query:\n",
    "        # Chnage mode\n",
    "        if token.lower() in logic_operators:\n",
    "            op = token.lower()\n",
    "            continue\n",
    "\n",
    "        # Token is not found\n",
    "        if token not in index:\n",
    "            continue\n",
    "        \n",
    "        url_list = set(index[token])\n",
    "\n",
    "        # Sets allow logic operations on \n",
    "        if op == \"or\":\n",
    "            result |= url_list  # If or join the two url lists\n",
    "        elif op == \"and\":\n",
    "            result &= url_list  # If and take the common links only\n",
    "        elif op == \"not\":\n",
    "            result -= url_list  # If not remove the links from the result\n",
    "\n",
    "    return result\n",
    "\n",
    "# Open the save file\n",
    "filepath = \"inverted_index.json\"\n",
    "with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "query = input(\"Request query: \")\n",
    "sites = request_query(query, data)\n",
    "\n",
    "for res in sites:\n",
    "    print(res) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### β) Κατάταξη αποτελεσμάτων (Ranking):\n",
    "Σε αυτή την υλοποίηση, ο χρήστης έχει την δυνατότητα να επιλέξει ανάμεσα σε τρείς αλγορίθμους ανάκτησης δεδομένων.<br><br>\n",
    "    **1. Boolean Retrieval <br>**\n",
    "    Ο αλγόριθμος είναι σε μεγάλο βαθμό ίδιος με την παραπάνω υλοποίηση με τη διαφορά οτι τα αποτελέσματα του ταξινομούνται χρησιμοποιώντας τον αλγόριθμο κατάταξης TF-IDF<br><br>\n",
    "    **2. Vector Space Model <br>**\n",
    "    O αλγοριθμός ανακτήσει τα δεδομένα χρησιμοποιόντας ένα πίνακα συχνότητων από τις λέξεις των εγγράφων, για αυτό δεν δέχεται το inverted index ως είσοδο αλλά το parsed scrape καθώς είναι πιο εύκολο να χτίσει έτσι τον πίνακα.<br><br>\n",
    "    **3. Okapi BM25 <br>**\n",
    "    Είναι ένας αλγόριθμος που κάνει κατάταξη βάση των πιθανοτήτων απο τις λέξεις του ερωτήματος, και τις λέξεις των εγγράφων.\n",
    "\n",
    "Στη παρακάτω δοκιμή τρέξαμε το Query: *Pacific and not Asia* με αλγόριθμο VSM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options:\n",
      "0. Exit\n",
      "1. Boolean Retrieval\n",
      "2. Vector Space Model (TF-IDF Ranking)\n",
      "3. Okapi BM25\n",
      "\n",
      "Results: \n",
      "URL: https://en.wikipedia.org/wiki/Template:Campaignbox_World_War_II. Score: 0.525416978120242\n",
      "URL: https://en.wikipedia.org/wiki/South-East_Asian_theatre_of_World_War_II. Score: 0.13196562210054344\n",
      "URL: https://en.wikipedia.org/wiki/Pacific_War. Score: 0.10440782793030888\n",
      "URL: https://en.wikipedia.org/wiki/World_War_II. Score: 0.08540729649592382\n",
      "URL: https://en.wikipedia.org/wiki/Indian_Ocean_in_World_War_II. Score: 0.05381481754044277\n",
      "URL: https://en.wikipedia.org/wiki/Empire_of_Japan. Score: 0.049830418553484836\n",
      "URL: https://en.wikipedia.org/wiki/Template_talk:Campaignbox_World_War_II. Score: 0.03807856292675406\n",
      "URL: https://en.wikipedia.org/wiki/American_Theater_(World_War_II). Score: 0.03437410663691899\n",
      "URL: https://en.wikipedia.org/wiki/World_War_II_by_country. Score: 0.0324988986782311\n",
      "URL: https://en.wikipedia.org/wiki/Allies_of_World_War_II. Score: 0.029072793902156127\n",
      "URL: https://en.wikipedia.org/wiki/Axis_powers. Score: 0.028481672910736517\n",
      "URL: https://en.wikipedia.org/wiki/Theater_(warfare). Score: 0.028154572269035453\n",
      "URL: https://en.wikipedia.org/wiki/Japanese_occupation_of_the_Philippines. Score: 0.027491947687725025\n",
      "URL: https://en.wikipedia.org/wiki/Air_raids_on_Japan. Score: 0.026902699863028984\n",
      "URL: https://en.wikipedia.org/wiki/Balkans_campaign_(World_War_II). Score: 0.02353216734856276\n",
      "URL: https://en.wikipedia.org/wiki/Aftermath_of_World_War_II. Score: 0.02076418141009671\n",
      "URL: https://en.wikipedia.org/wiki/Second_Sino-Japanese_War. Score: 0.018606737306621445\n",
      "URL: https://en.wikipedia.org/wiki/Hirohito. Score: 0.01676401267055789\n",
      "URL: https://en.wikipedia.org/wiki/United_States. Score: 0.015764738390241777\n",
      "URL: https://en.wikipedia.org/wiki/Soviet_Union. Score: 0.015110723940251855\n",
      "URL: https://en.wikipedia.org/wiki/North_African_campaign. Score: 0.013167420965179328\n",
      "URL: https://en.wikipedia.org/wiki/Phoney_War. Score: 0.01287651981558131\n",
      "URL: https://en.wikipedia.org/wiki/Franklin_D._Roosevelt. Score: 0.012821396088374053\n",
      "URL: https://en.wikipedia.org/wiki/Joseph_Stalin. Score: 0.010119180255648874\n",
      "URL: https://en.wikipedia.org/wiki/Operation_Weser%C3%BCbung. Score: 0.00988242370468125\n",
      "URL: https://en.wikipedia.org/wiki/Mediterranean_and_Middle_East_theatre_of_World_War_II. Score: 0.009300803261118085\n",
      "URL: https://en.wikipedia.org/wiki/Western_Front_(World_War_II). Score: 0.008614219984757136\n",
      "URL: https://en.wikipedia.org/wiki/Western_Front_(World_War_II)#1944–1945:_The_Second_Front. Score: 0.008614219984757136\n",
      "URL: https://en.wikipedia.org/wiki/Atomic_bombings_of_Hiroshima_and_Nagasaki. Score: 0.008343676268180833\n",
      "URL: https://en.wikipedia.org/wiki/Eastern_Front_(World_War_II). Score: 0.00794337646475921\n",
      "URL: https://en.wikipedia.org/wiki/World_War_II_casualties. Score: 0.00784477376636394\n",
      "URL: https://en.wikipedia.org/wiki/Lapland_War. Score: 0.007119824638237501\n",
      "URL: https://en.wikipedia.org/wiki/Invasion_of_Poland. Score: 0.007086834989363794\n",
      "URL: https://en.wikipedia.org/wiki/European_theatre_of_World_War_II. Score: 0.006017756103390405\n",
      "URL: https://en.wikipedia.org/wiki/Matilda_II. Score: 0.005999017928030828\n",
      "URL: https://en.wikipedia.org/wiki/Invasion_of_Lingayen_Gulf. Score: 0.005439640326971187\n",
      "URL: https://en.wikipedia.org/wiki/Battle_of_the_Atlantic. Score: 0.005420846047635918\n",
      "URL: https://en.wikipedia.org/wiki/United_Kingdom. Score: 0.005176254250738593\n",
      "URL: https://en.wikipedia.org/wiki/Republic_of_China_(1912%E2%80%931949). Score: 0.004023658088823537\n",
      "URL: https://en.wikipedia.org/wiki/Continuation_War. Score: 0.0031385644963866435\n",
      "URL: https://en.wikipedia.org/wiki/East_African_campaign_(World_War_II). Score: 0.002830535362226409\n",
      "URL: https://en.wikipedia.org/wiki/Winter_War. Score: 0.0026533470249495202\n",
      "URL: https://en.wikipedia.org/wiki/Winston_Churchill. Score: 0.0026278546386995067\n",
      "URL: https://en.wikipedia.org/wiki/Battle_of_France. Score: 0.002497895087399906\n",
      "URL: https://en.wikipedia.org/wiki/Battle_of_Britain. Score: 0.002310518142121582\n",
      "URL: https://en.wikipedia.org/wiki/Adolf_Hitler. Score: 0.0016138612594765159\n",
      "URL: https://en.wikipedia.org/wiki/Benito_Mussolini. Score: 0.001430809433284707\n",
      "URL: https://en.wikipedia.org/wiki/Chiang_Kai-shek. Score: 0.0009128880909392158\n",
      "URL: https://en.wikipedia.org/wiki/WWII_(disambiguation). Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/The_Second_World_War_(disambiguation). Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/World_War_II_(disambiguation). Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Junkers_Ju_87. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Battle_of_Stalingrad. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Raising_a_Flag_over_the_Reichstag. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Reichstag_building. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Battle_of_Berlin. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Battle_of_Gabon. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Attacks_on_Australia_during_World_War_II. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Battle_of_the_Caribbean. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Allied_leaders_of_World_War_II. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Axis_leaders_of_World_War_II. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Nazi_Germany. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Fascist_Italy. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Special:EditPage/Template:Campaignbox_World_War_II. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Polish_resistance_movement_in_World_War_II. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Finland_in_World_War_II. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/French_Resistance. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Italian_invasion_of_France. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Second_Battle_of_the_Alps. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Yugoslav_Partisans. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Greek_resistance. Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "logic_operators = {\"and\", \"or\", \"not\"}\n",
    "\n",
    "def preprocess_query(query: str, exclude_words: set = set()) -> str:\n",
    "\n",
    "    # Init nltk objects\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english')) - exclude_words\n",
    "\n",
    "    # Tokenize query\n",
    "    tokens = word_tokenize(query.lower())\n",
    "    # Remove all the stop words inside the query tokens\n",
    "    non_stopwords_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    # Lematize the remain words\n",
    "    lemmatized_query = \" \".join([lemmatizer.lemmatize(word) for word in non_stopwords_tokens])\n",
    "\n",
    "    return lemmatized_query\n",
    "\n",
    "def ranking_TF_IDF(parsed_scrape: dict, query: str, result_set: set = None):\n",
    "\n",
    "    if not query:\n",
    "        return {}\n",
    "\n",
    "    # Preprocess the query\n",
    "    lemmatized_query = preprocess_query(query, logic_operators)\n",
    "\n",
    "    # Combine the URL and the paragraphs in a signle line\n",
    "    documents = {entry['website_url']: \" \".join(entry['content']) for entry in parsed_scrape}\n",
    "    \n",
    "    # A Result set has been provided\n",
    "    if result_set is not None:\n",
    "\n",
    "        # Remove documents that arent in the result set\n",
    "        documents = {url: content for url, content in documents.items() if url in result_set}\n",
    "\n",
    "    # Init the TF-IDF matrix\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents.values())\n",
    "\n",
    "    query_vector = vectorizer.transform([lemmatized_query])\n",
    "\n",
    "    scores = cosine_similarity(query_vector, tfidf_matrix).flatten()  # Using flatter to conv to vectoer\n",
    "    ranked_results = sorted(zip(documents.keys(), scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked_results\n",
    "    \n",
    "def boolean_retrieval(query: str, index: dict) -> set:\n",
    "    \n",
    "    if not query: \n",
    "        return {}\n",
    "    \n",
    "    # Preprossess query to remove any unwanted words or characters, keeping the logic ops\n",
    "    lemmatized_query = preprocess_query(query, logic_operators)\n",
    "\n",
    "    # Remove any duplicates\n",
    "    result = set()\n",
    "\n",
    "    # Default search op is logic or\n",
    "    op = \"or\"\n",
    "\n",
    "    for token in lemmatized_query.split():\n",
    "        # Chnage mode\n",
    "        if token.lower() in logic_operators:\n",
    "            op = token.lower()\n",
    "            continue\n",
    "\n",
    "        # Token is not found\n",
    "        if token not in index:\n",
    "            continue\n",
    "        \n",
    "        url_list = set(index[token])\n",
    "\n",
    "        # Sets allow logic operations on pyth\n",
    "        if op == \"or\":\n",
    "            result |= url_list  # If or join the two url lists\n",
    "        elif op == \"and\":\n",
    "            result &= url_list  # If and take the common links only\n",
    "        elif op == \"not\":\n",
    "            result -= url_list  # If not remove the links from the result\n",
    "\n",
    "    return result\n",
    "\n",
    "def vsm_retrieval(query: str, parsed_scrape: dict):\n",
    "    if not query:\n",
    "        return {}\n",
    "\n",
    "    processed_query = preprocess_query(query)\n",
    "\n",
    "    results = ranking_TF_IDF(parsed_scrape, processed_query)\n",
    "    return results\n",
    "\n",
    "def probabilistic_retrieval(parsed_scrape: list, query: str):\n",
    "    if not query:\n",
    "        return []\n",
    "\n",
    "    # Preprocess the query\n",
    "    processed_query = preprocess_query(query)\n",
    "\n",
    "    # Combine the URL and the paragraphs in a signle line\n",
    "    documents = {entry['website_url']: \" \".join(entry['content']) for entry in parsed_scrape}\n",
    "\n",
    "    # Tokenize documents\n",
    "    tokenized_documents = [word_tokenize(doc.lower()) for doc in documents.values()]\n",
    "\n",
    "    # Initialize BM25 model\n",
    "    bm25 = BM25Okapi(tokenized_documents)\n",
    "\n",
    "    # Tokenize the query\n",
    "    tokenized_query = word_tokenize(processed_query)\n",
    "\n",
    "    # Get BM25 scores\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    ranked_results = sorted(zip(documents.keys(),scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked_results\n",
    "\n",
    "def dataRetrival(inverted_index: dict, parsed_scrape: dict, option: str, query: str) -> set:\n",
    "    result_set = set()\n",
    "    if option == \"1\":\n",
    "        bool_results = boolean_retrieval(query, inverted_index)\n",
    "        result_set = ranking_TF_IDF(parsed_scrape, query, bool_results)\n",
    "    elif option == \"2\":\n",
    "        result_set = vsm_retrieval(query, parsed_scrape)\n",
    "    elif option == \"3\":\n",
    "        result_set = probabilistic_retrieval(parsed_scrape,query)\n",
    "    else:\n",
    "        raise Exception(f\"Invalid Retrival Method: \\\"{option}\\\"\")\n",
    "    \n",
    "    return result_set\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Open the inverted index save file\n",
    "    with open('inverted_index.json', 'r') as file:\n",
    "        inverted_index = json.load(file)\n",
    "\n",
    "    # Open the parsed data (Used for TF-IDF matrix init)\n",
    "    with open('parsed_scrape.json', 'r') as file:\n",
    "        parsed_scrape = json.load(file)\n",
    "\n",
    "    print(\"Options:\")\n",
    "    print(\"0. Exit\")\n",
    "    print(\"1. Boolean Retrieval\")\n",
    "    print(\"2. Vector Space Model (TF-IDF Ranking)\")\n",
    "    print(\"3. Okapi BM25\")\n",
    "    option = input(\"0,1,2,3: \")\n",
    "    if option == \"0\":\n",
    "        exit()\n",
    "\n",
    "    query = input(\"Request query: \")\n",
    "\n",
    "    result_set = dataRetrival(inverted_index, parsed_scrape, option, query)\n",
    "\n",
    "    print(\"\\nResults: \")\n",
    "    for score, url in result_set:\n",
    "        print(f\"URL: {score}. Score: {url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 5. Αξιολόγηση συστήματος:\n",
    "Για την αξιολόγηση του συστήματος έχει δημιουργηθεί μια δομή με *καθολικές αλήθεις (Ground Truths)* η οποία χρησιμοποιείται ως σημείο αναφοράς για την αξιολόγηση. <br>\n",
    "Στο πρόγραμμα αξιολόγησης ο χρήστης μπορεί να επιλέξει τον αλγόριθμο που θέλει να αξιολογήσει και το πρόγραμμα θα τρέξει τα Queries που υπάρχουν στο Ground Truths χρησιμοποιόντας τη συνάρτηση ανάκτησης απο το παραπάνω ερώτημα και ύστερα συγκρίνοντας τα αποτελέσματα της αναζήτησης με το Ground Truths, παράγει τις τιμές αξιολόγησης."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ground_truths.json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"query\": \"Junkers Ju 87\",\n",
    "        \"links\": [\n",
    "            \"https://en.wikipedia.org/wiki/Junkers_Ju_87\",\n",
    "            \"https://en.wikipedia.org/wiki/Battle_of_Britain\",\n",
    "            \"https://en.wikipedia.org/wiki/Battle_of_France\",\n",
    "            \"https://en.wikipedia.org/wiki/Invasion_of_Poland\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Major Battles\",\n",
    "        \"links\": [\n",
    "            \"https://en.wikipedia.org/wiki/Battle_of_Stalingrad\",\n",
    "            \"https://en.wikipedia.org/wiki/Allies_of_World_War_II\",\n",
    "            \"https://en.wikipedia.org/wiki/Second_Sino-Japanese_War\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"United Kingdom and not Germany\",\n",
    "        \"links\": [\n",
    "            \"https://en.wikipedia.org/wiki/South-East_Asian_theatre_of_World_War_II\",\n",
    "            \"https://en.wikipedia.org/wiki/Japanese_occupation_of_the_Philippines\",\n",
    "            \"https://en.wikipedia.org/wiki/Attacks_on_Australia_during_World_War_II\",\n",
    "            \"https://en.wikipedia.org/wiki/Matilda_II\",\n",
    "            \"https://en.wikipedia.org/wiki/Invasion_of_Lingayen_Gulf\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"United States in the Pacific\",\n",
    "        \"links\": [\n",
    "            \"https://en.wikipedia.org/wiki/United_States\",\n",
    "            \"https://en.wikipedia.org/wiki/Allies_of_World_War_II\",\n",
    "            \"https://en.wikipedia.org/wiki/World_War_II\",\n",
    "            \"https://en.wikipedia.org/wiki/Aftermath_of_World_War_II\",\n",
    "            \"https://en.wikipedia.org/wiki/Axis_powers\",\n",
    "            \"https://en.wikipedia.org/wiki/Soviet_Union\",\n",
    "            \"https://en.wikipedia.org/wiki/South-East_Asian_theatre_of_World_War_II\",\n",
    "            \"https://en.wikipedia.org/wiki/Pacific_War\",\n",
    "            \"https://en.wikipedia.org/wiki/American_theater_(World_War_II)\",\n",
    "            \"https://en.wikipedia.org/wiki/Empire_of_Japan\",\n",
    "            \"https://en.wikipedia.org/wiki/Japanese_occupation_of_the_Philippines\"\n",
    "        ]\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Η κλάση `EvaluationValues` χρησιμοποιείται για ευκολότερη μεταφορά δεδομένω και εμφάνηση των αποτελεσμάτων*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Evaluation Algorithm\n",
      "0. Exit\n",
      "1. Boolean Retrieval\n",
      "2. Vector Space Model (TF-IDF Ranking)\n",
      "3. Okapi BM25\n",
      "\n",
      "Query: Junkers Ju 87. Evaluation Values:\n",
      "Precission: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "Map: 1.0\n",
      "\n",
      "Query: Major Battles. Evaluation Values:\n",
      "Precission: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "Map: 0.04838709677419355\n",
      "\n",
      "Query: United Kingdom and not Germany. Evaluation Values:\n",
      "Precission: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "Map: 0.078125\n",
      "\n",
      "Query: United States in the Pacific. Evaluation Values:\n",
      "Precission: 0.7\n",
      "Recall: 0.7\n",
      "F1 Score: 0.7\n",
      "Map: 0.5361538461538461\n"
     ]
    }
   ],
   "source": [
    "#import search_engine_2     # Use this outside Jupyter nodebook\n",
    "import json\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# Data transfer class for the metrics\n",
    "class EvaluationValues:\n",
    "    _Precision: float\n",
    "    _Recall: float\n",
    "    _F1_score: float\n",
    "    _Map: float\n",
    "\n",
    "    # Print method\n",
    "    def print_values(self):\n",
    "        print(f\"Precission: {self._Precision}\")\n",
    "        print(f\"Recall: {self._Recall}\")\n",
    "        print(f\"F1 Score: {self._F1_score}\")\n",
    "        print(f\"Map: {self._Map}\")\n",
    "\n",
    "\n",
    "def evaluate_query(results, ground_truth_set: set) -> EvaluationValues:\n",
    "    relative_results = []\n",
    "\n",
    "    # Keep only reletive docs\n",
    "    for link, score in results:\n",
    "        if score != 0:\n",
    "            relative_results.append(link)\n",
    "\n",
    "    # Convert to tables for the sklearn lib\n",
    "    y_true = [1 if link in ground_truth_set else 0 for link in relative_results]    \n",
    "    y_pred = [1 if link in relative_results else 0 for link in ground_truth_set]\n",
    "\n",
    "    # Pad with 0 \n",
    "    while len(y_pred) != len(y_true):\n",
    "        y_pred.append(0)\n",
    "\n",
    "    results = EvaluationValues()\n",
    "    results._Precision = metrics.precision_score(y_true, y_pred)\n",
    "    results._Recall = metrics.recall_score(y_true, y_pred)\n",
    "    results._F1_score = metrics.f1_score(y_true, y_pred)\n",
    "    results._Map = metrics.average_precision_score(y_true, y_pred)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Open the inverted index save file\n",
    "    with open('inverted_index.json', 'r') as file:\n",
    "        inverted_index = json.load(file)\n",
    "\n",
    "    # Open the parsed data (Used for TF-IDF matrix init)\n",
    "    with open('parsed_scrape.json', 'r') as file:\n",
    "        parsed_scrape = json.load(file)\n",
    "    \n",
    "    # Use this outside Jupyter notebook\n",
    "    #with open('ground_truths.json', 'r') as file:\n",
    "    #   data = json.load(file)\n",
    "\n",
    "    # Select the algorithm\n",
    "    print(\"Select Evaluation Algorithm\")\n",
    "    print(\"0. Exit\")\n",
    "    print(\"1. Boolean Retrieval\")\n",
    "    print(\"2. Vector Space Model (TF-IDF Ranking)\")\n",
    "    print(\"3. Okapi BM25\")\n",
    "    \n",
    "    option = input(\"0,1,2,3: \")\n",
    "    \n",
    "    if option == \"0\":\n",
    "        exit()\n",
    "    elif option != \"1\" and option != \"2\" and option != \"3\":\n",
    "        raise Exception(f\"Invalid option: {option}\")\n",
    "\n",
    "    for index, question in enumerate(data):\n",
    "        query = question['query']\n",
    "        ground_truths = set(question['links'])\n",
    "\n",
    "        # Call the data retrival func from the previuse question\n",
    "        #result_set = search_engine_2.dataRetrival(inverted_index, parsed_scrape, option, query)    # Use this outside Jupyter notebook\n",
    "        result_set = dataRetrival(inverted_index, parsed_scrape, option, query)     # Use this inside Jupyter notebook\n",
    "\n",
    "        ev = evaluate_query(result_set, ground_truths)\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"\\nQuery: {query}. Evaluation Values:\")\n",
    "        ev.print_values()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
