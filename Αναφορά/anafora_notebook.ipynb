{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ανάκτηση Πληροφορίας\n",
    "\n",
    "Σουκαράς Σωτήριος ice21390206\n",
    "Θεοφάνης Κουνιάκης ice21390103\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 1: Συλλογή δεδομένων\n",
    "\n",
    "Για τη συλλογή δεδομένων έχει καταστευάσετει ένα αναδρομικό προγράμμα Data Scrape για Wikipedia. Το πρόγραμμα εξερευνεί αναδρομικά όλους τους συνδέσμους που περιέχονται μέσα σε μια σελίδα της Wikipedia μέχρι ένα συγκεκριμένο βάθος.\n",
    "\n",
    "**To ID \"mw-content-text\"** <br>\n",
    "Στις σελίδες της Wikipedia όλο το ουσιαστικό περιεχόμενο περιέχεται μέσα στο id \"mw-content-text\". Επομένος δεν εξερευνούνται links που οδηγούν στην αρχική σελιδα της Wikipedia για παράδειγμα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_wikipedia(URL, depth_limit, depth = 1):\n",
    "    parsed_paragraphs = {}\n",
    "    print(\"Searching \" + URL + \"...\")\n",
    "    \n",
    "    try:\n",
    "        wiki_responce = requests.get(URL)\n",
    "        wiki_responce.raise_for_status()        # Throw if error was encountered in the request\n",
    "\n",
    "        # Parse the responce with BeautifulSoup\n",
    "        soup_responce = BeautifulSoup(wiki_responce.text, 'html.parser')\n",
    "        soup_paragraphs = soup_responce.find_all('p')\n",
    "\n",
    "        # Remove html tags and append them to the return values if they have text\n",
    "        parsed_paragraphs[URL] = [p.text.strip() for p in soup_paragraphs if p.text.strip() != \"\"]\n",
    "\n",
    "        # If the maxt depth of the search has been reached exit the recursion\n",
    "        if depth >= depth_limit:\n",
    "            return parsed_paragraphs\n",
    "        \n",
    "        # Find the main content of the wiki article if exists\n",
    "        body = soup_responce.find(id=\"mw-content-text\")\n",
    "        if not body:\n",
    "            return parsed_paragraphs\n",
    "        \n",
    "        for link in body.find_all('a'):\n",
    "            # If the href tag in not present or it doesn't point to an other wiki side skip it\n",
    "            if not ('href' in link.attrs) or link['href'].find(\"/wiki/\") == -1 or link['href'].find(\"File:\") != -1:\n",
    "                continue\n",
    "\n",
    "            # Search the next wiki link\n",
    "            new_paragraphs = fetch_wikipedia(\"https://en.wikipedia.org\" + link['href'], depth_limit, depth + 1)\n",
    "            # Dont spam the wiki database\n",
    "            time.sleep(1)       \n",
    "\n",
    "            # Return value is valid\n",
    "            if not new_paragraphs:\n",
    "                continue\n",
    "            \n",
    "            # Append the return values to the dictionary \n",
    "            parsed_paragraphs.update(new_paragraphs)\n",
    "\n",
    "        return parsed_paragraphs\n",
    "    except:\n",
    "        print(\"Unable to parse link: \" + URL)\n",
    "        return parsed_paragraphs\n",
    "\n",
    "\n",
    "#Fetch info for link with max recusive search of 2\n",
    "results = fetch_wikipedia(\"https://en.wikipedia.org/wiki/World_War_II\", 2)\n",
    "\n",
    "filename = \"wiki_scrape.json\"\n",
    "\n",
    "# Convert to json object\n",
    "json_object = [\n",
    "    {\n",
    "        \"website_url\": website,\n",
    "        \"content\": data_list, \n",
    "    }\n",
    "    for website, data_list in results.items()\n",
    "]\n",
    "\n",
    "# Save as JSON file\n",
    "try:\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(json_object, file, indent=4)\n",
    "    print(f\"Data saved to JSON file: {filename}\")\n",
    "except IOError as e:\n",
    "    print(f\"Error saving to JSON file: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 2. Προεπεξεργασία κειμένου (Text Processing):\n",
    "Για την προεπεξεργασία του κειμένου, δημιουργήθηκε πρόγραμμα που να παίρνει το .json αρχείο του βήματος 1 και:\n",
    "1) χωρίζει τα κείμενα σε λέξεις (tokenization), \n",
    "2) αφαιρεί τα stop words (πχ 'is', 'the', 'or'),\n",
    "3) και μετατρέπει κάθε λέξη σε λεξικογραφική μορφή (lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
