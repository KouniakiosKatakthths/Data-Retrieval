{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ανάκτηση Πληροφορίας\n",
    "\n",
    "Σουκαράς Σωτήριος ice21390206\n",
    "Θεοφάνης Κουνιάκης ice21390103\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 1: Συλλογή δεδομένων\n",
    "\n",
    "Για τη συλλογή δεδομένων έχει καταστευάσετει ένα αναδρομικό προγράμμα Data Scrape για Wikipedia. Το πρόγραμμα εξερευνεί αναδρομικά όλους τους συνδέσμους που περιέχονται μέσα σε μια σελίδα της Wikipedia μέχρι ένα συγκεκριμένο βάθος. Τα αποτελέσματα αποθηκεύονται στο αρχείο \"wiki_scrape.json\" σε μορφή οπού κάθε json αντικείμενο περιέχει το Link του ιστότοπου και τις παραγρφαφούς του ιστότοπου σε μια λίστα.\n",
    "\n",
    "**To ID \"mw-content-text\"** <br>\n",
    "Στις σελίδες της Wikipedia όλο το ουσιαστικό περιεχόμενο περιέχεται μέσα στο id \"mw-content-text\". Επομένος δεν εξερευνούνται links που οδηγούν στην αρχική σελιδα της Wikipedia για παράδειγμα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching https://en.wikipedia.org/wiki/World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/WWII_(disambiguation)...\n",
      "Searching https://en.wikipedia.org/wiki/The_Second_World_War_(disambiguation)...\n",
      "Searching https://en.wikipedia.org/wiki/World_War_II_(disambiguation)...\n",
      "Searching https://en.wikipedia.org/wiki/Junkers_Ju_87...\n",
      "Searching https://en.wikipedia.org/wiki/Eastern_Front_(World_War_II)...\n",
      "Searching https://en.wikipedia.org/wiki/Matilda_II...\n",
      "Searching https://en.wikipedia.org/wiki/North_African_campaign...\n",
      "Searching https://en.wikipedia.org/wiki/Atomic_bombings_of_Hiroshima_and_Nagasaki...\n",
      "Searching https://en.wikipedia.org/wiki/Battle_of_Stalingrad...\n",
      "Unable to parse link: https://en.wikipedia.org/wiki/Battle_of_Stalingrad\n",
      "Searching https://en.wikipedia.org/wiki/Raising_a_Flag_over_the_Reichstag...\n",
      "Searching https://en.wikipedia.org/wiki/Reichstag_building...\n",
      "Searching https://en.wikipedia.org/wiki/Battle_of_Berlin...\n",
      "Searching https://en.wikipedia.org/wiki/Invasion_of_Lingayen_Gulf...\n",
      "Searching https://en.wikipedia.org/wiki/Japanese_occupation_of_the_Philippines...\n",
      "Searching https://en.wikipedia.org/wiki/Theater_(warfare)...\n",
      "Searching https://en.wikipedia.org/wiki/European_theatre_of_World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/Pacific_War...\n",
      "Searching https://en.wikipedia.org/wiki/Battle_of_the_Atlantic...\n",
      "Searching https://en.wikipedia.org/wiki/Indian_Ocean_in_World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/South-East_Asian_theatre_of_World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/Second_Sino-Japanese_War...\n",
      "Searching https://en.wikipedia.org/wiki/Air_raids_on_Japan...\n",
      "Searching https://en.wikipedia.org/wiki/Mediterranean_and_Middle_East_theatre_of_World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/Mediterranean_and_Middle_East_theatre_of_World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/North_African_campaign...\n",
      "Searching https://en.wikipedia.org/wiki/East_African_campaign_(World_War_II)...\n",
      "Searching https://en.wikipedia.org/wiki/Battle_of_Gabon...\n",
      "Searching https://en.wikipedia.org/wiki/Attacks_on_Australia_during_World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/Battle_of_the_Caribbean...\n",
      "Searching https://en.wikipedia.org/wiki/American_Theater_(World_War_II)...\n",
      "Searching https://en.wikipedia.org/wiki/Allies_of_World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/Aftermath_of_World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/World_War_II_by_country...\n",
      "Searching https://en.wikipedia.org/wiki/Allies_of_World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/Axis_powers...\n",
      "Searching https://en.wikipedia.org/wiki/Allied_leaders_of_World_War_II...\n",
      "Searching https://en.wikipedia.org/wiki/Soviet_Union...\n",
      "Searching https://en.wikipedia.org/wiki/Joseph_Stalin...\n",
      "Searching https://en.wikipedia.org/wiki/United_States...\n",
      "Searching https://en.wikipedia.org/wiki/Franklin_D._Roosevelt...\n",
      "Searching https://en.wikipedia.org/wiki/United_Kingdom...\n",
      "Searching https://en.wikipedia.org/wiki/Winston_Churchill...\n",
      "Searching https://en.wikipedia.org/wiki/Republic_of_China_(1912%E2%80%931949)...\n",
      "Searching https://en.wikipedia.org/wiki/Chiang_Kai-shek...\n",
      "Searching https://en.wikipedia.org/wiki/Axis_leaders_of_World_War_II...\n",
      "Unable to parse link: https://en.wikipedia.org/wiki/World_War_II\n",
      "Data saved to JSON file: wiki_scrape.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_wikipedia(URL, depth_limit, depth = 1):\n",
    "    parsed_paragraphs = {}\n",
    "    print(\"Searching \" + URL + \"...\")\n",
    "    \n",
    "    try:\n",
    "        wiki_responce = requests.get(URL)\n",
    "        wiki_responce.raise_for_status()        # Throw if error was encountered in the request\n",
    "\n",
    "        # Parse the responce with BeautifulSoup\n",
    "        soup_responce = BeautifulSoup(wiki_responce.text, 'html.parser')\n",
    "        soup_paragraphs = soup_responce.find_all('p')\n",
    "\n",
    "        # Remove html tags and append them to the return values if they have text\n",
    "        parsed_paragraphs[URL] = [p.text.strip() for p in soup_paragraphs if p.text.strip() != \"\"]\n",
    "\n",
    "        # If the maxt depth of the search has been reached exit the recursion\n",
    "        if depth >= depth_limit:\n",
    "            return parsed_paragraphs\n",
    "        \n",
    "        # Find the main content of the wiki article if exists\n",
    "        body = soup_responce.find(id=\"mw-content-text\")\n",
    "        if not body:\n",
    "            return parsed_paragraphs\n",
    "        \n",
    "        for link in body.find_all('a'):\n",
    "            # If the href tag in not present or it doesn't point to an other wiki side skip it\n",
    "            if not ('href' in link.attrs) or link['href'].find(\"/wiki/\") == -1 or link['href'].find(\"File:\") != -1:\n",
    "                continue\n",
    "\n",
    "            # Search the next wiki link\n",
    "            new_paragraphs = fetch_wikipedia(\"https://en.wikipedia.org\" + link['href'], depth_limit, depth + 1)\n",
    "            # Dont spam the wiki database\n",
    "            time.sleep(1)       \n",
    "\n",
    "            # Return value is valid\n",
    "            if not new_paragraphs:\n",
    "                continue\n",
    "            \n",
    "            # Append the return values to the dictionary \n",
    "            parsed_paragraphs.update(new_paragraphs)\n",
    "\n",
    "        return parsed_paragraphs\n",
    "    except:\n",
    "        print(\"Unable to parse link: \" + URL)\n",
    "        return parsed_paragraphs\n",
    "\n",
    "\n",
    "#Fetch info for link with max recusive search of 2\n",
    "results = fetch_wikipedia(\"https://en.wikipedia.org/wiki/World_War_II\", 2)\n",
    "\n",
    "filename = \"wiki_scrape.json\"\n",
    "\n",
    "# Convert to json object\n",
    "json_object = [\n",
    "    {\n",
    "        \"website_url\": website,\n",
    "        \"content\": data_list, \n",
    "    }\n",
    "    for website, data_list in results.items()\n",
    "]\n",
    "\n",
    "# Save as JSON file\n",
    "try:\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(json_object, file, indent=4)\n",
    "    print(f\"Data saved to JSON file: {filename}\")\n",
    "except IOError as e:\n",
    "    print(f\"Error saving to JSON file: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Setup\n",
    "Για τη σωστή λειουργεία των υπόλοιπων προγραμμάτων απαιτείται η βιβλιοθήκη NLTK. Ο παρακάτων κώδικας ελένχει και κατεβά\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download()\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 2. Προεπεξεργασία κειμένου (Text Processing):\n",
    "Για την προεπεξεργασία του κειμένου, δημιουργήθηκε πρόγραμμα που να παίρνει το .json αρχείο του βήματος 1 και:\n",
    "1) χωρίζει τα κείμενα σε λέξεις (tokenization), \n",
    "2) αφαιρεί τα stop words (πχ 'is', 'the', 'or'),\n",
    "3) και μετατρέπει κάθε λέξη σε λεξικογραφική μορφή (lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re       #REGEX\n",
    "\n",
    "import nltk.stem\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def clear_special_char(text: str) -> str:\n",
    "    brackets_regex = r\"\\[[^\\]]*\\]\"\n",
    "    alpharethmetic_regex = r\"[^a-zA-Z0-9\\s]\"\n",
    "\n",
    "    # Remove the references like [55] or [a]\n",
    "    parsed_text = re.sub(brackets_regex, \"\", text)\n",
    "\n",
    "    # The '-' many times is used as seperator to seperate the words with a ' '\n",
    "    parsed_text = re.sub('-', \" \", parsed_text)\n",
    "    # Remove any non alapharithmetic char\n",
    "    parsed_text = re.sub(alpharethmetic_regex, \"\", parsed_text)\n",
    "\n",
    "    parsed_text.strip()\n",
    "\n",
    "    return parsed_text\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    # Lemmatizer and stop word objects for english\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    # Tokenize the paragraph\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove all the words inside the stop word container\n",
    "    non_stopwords_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Lematize the remain words\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in non_stopwords_tokens]\n",
    "\n",
    "    # Join all the lemmatized words into a string\n",
    "    final_text = \" \".join(lemmatized)\n",
    "    return final_text\n",
    "\n",
    "# Open the save file\n",
    "filepath = \"wiki_scrape.json\"\n",
    "with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Parse all the text in the scrape file\n",
    "parsed_data = {}\n",
    "for index, site_data in enumerate(data):\n",
    "    parsed_content = []\n",
    "    link = site_data[\"website_url\"]\n",
    "    for p_index, paragraph in enumerate(site_data[\"content\"]):\n",
    "        # For each paragraph run clean and preprocess\n",
    "        clear_paragraph = clear_special_char(paragraph)\n",
    "        parsed_paragraph = preprocess_text(clear_paragraph)\n",
    "        parsed_content.append(parsed_paragraph)\n",
    "    \n",
    "    parsed_data[link] = parsed_content\n",
    "\n",
    "# Convert to json object\n",
    "json_object = [\n",
    "    {\n",
    "        \"website_url\": website,\n",
    "        \"content\": data_list, \n",
    "    }\n",
    "    for website, data_list in parsed_data.items()\n",
    "]\n",
    "\n",
    "# Save as JSON file\n",
    "filename = \"parsed_scrape.json\"\n",
    "try:\n",
    "    with open(filename, \"w\") as file:\n",
    "        json.dump(json_object, file, indent=4)\n",
    "    print(f\"Data saved to JSON file: {filename}\")\n",
    "except IOError as e:\n",
    "    print(f\"Error saving to JSON file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 3. Ευρετήριο (Indexing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index saved to: inverted_index.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def create_inverted_index(data):\n",
    "    inverted_index = defaultdict(list)\n",
    "\n",
    "    # For all the enties in the file\n",
    "    for index, site_data in enumerate(data):\n",
    "        link = site_data[\"website_url\"]\n",
    "\n",
    "        # For each paragraph of the entry\n",
    "        for p_index, paragraph in enumerate(site_data[\"content\"]):\n",
    "            # Remove any caps\n",
    "            paragraph = paragraph.lower();\n",
    "\n",
    "            # Take only the unique words inside the paragraph\n",
    "            # Split them using the ' '\n",
    "            words = set(paragraph.split()) \n",
    "            \n",
    "            # For every unique word in paragraph\n",
    "            for word in words:\n",
    "                # If the website link is not present in the on the word entry append it  \n",
    "                if link not in inverted_index[word]:\n",
    "                    inverted_index[word].append(link)\n",
    "\n",
    "    return inverted_index\n",
    "\n",
    "# Open the save file\n",
    "filepath = \"parsed_scrape.json\"\n",
    "with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "inverted_index = create_inverted_index(data)\n",
    "\n",
    "# Convert to dict\n",
    "inverted_index = dict(inverted_index)\n",
    "\n",
    "# Save to JSON\n",
    "index_filename = \"inverted_index.json\"\n",
    "try:\n",
    "    with open(index_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(inverted_index, file, ensure_ascii=False, indent=4)\n",
    "    print(f\"Inverted index saved to: {index_filename}\")\n",
    "except IOError as e:\n",
    "    print(f\"Error saving inverted index: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 4. Μηχανή αναζήτησης (Search Engine):\n",
    "#### α) Επεξεργασία ερωτήματος (Query Processing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Junkers_Ju_87\n",
      "https://en.wikipedia.org/wiki/South-East_Asian_theatre_of_World_War_II\n",
      "https://en.wikipedia.org/wiki/American_Theater_(World_War_II)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# NLTK imports\n",
    "import nltk.stem\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def request_query(query: str, index: dict) -> set:\n",
    "    logic_operators = {\"and\", \"or\", \"not\"};\n",
    "\n",
    "    # Init nltk objects\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english')) - logic_operators\n",
    "\n",
    "    # Tokenize query\n",
    "    tokens = word_tokenize(query.lower())\n",
    "\n",
    "    # Remove all the stop words inside the query tokens\n",
    "    non_stopwords_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Lematize the remain words\n",
    "    lemmatized_query = [lemmatizer.lemmatize(word) for word in non_stopwords_tokens];\n",
    "\n",
    "    # Remove any duplicates\n",
    "    result = set()\n",
    "\n",
    "    # Default search op is logic or\n",
    "    op = \"or\"\n",
    "\n",
    "    for token in lemmatized_query:\n",
    "        # Chnage mode\n",
    "        if token.lower() in logic_operators:\n",
    "            op = token.lower()\n",
    "            continue\n",
    "\n",
    "        # Token is not found\n",
    "        if token not in index:\n",
    "            continue\n",
    "        \n",
    "        url_list = set(index[token])\n",
    "\n",
    "        # Sets allow logic operations on \n",
    "        if op == \"or\":\n",
    "            result |= url_list  # If or join the two url lists\n",
    "        elif op == \"and\":\n",
    "            result &= url_list  # If and take the common links only\n",
    "        elif op == \"not\":\n",
    "            result -= url_list  # If not remove the links from the result\n",
    "\n",
    "    return result\n",
    "\n",
    "# Open the save file\n",
    "filepath = \"inverted_index.json\"\n",
    "with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "query = input(\"Request query: \")\n",
    "sites = request_query(query, data)\n",
    "\n",
    "for res in sites:\n",
    "    print(res) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### β) Κατάταξη αποτελεσμάτων (Ranking):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options:\n",
      "0. Exit\n",
      "1. Boolean Retrieval\n",
      "2. Vector Space Model (TF-IDF Ranking)\n",
      "3. Okapi BM25\n",
      "\n",
      "Results: \n",
      "URL: https://en.wikipedia.org/wiki/United_States. Score: 0.005236200193535844\n",
      "URL: https://en.wikipedia.org/wiki/Invasion_of_Lingayen_Gulf. Score: 0.0048057257166579\n",
      "URL: https://en.wikipedia.org/wiki/Atomic_bombings_of_Hiroshima_and_Nagasaki. Score: 0.00465422889245384\n",
      "URL: https://en.wikipedia.org/wiki/East_African_campaign_(World_War_II). Score: 0.004248893720205598\n",
      "URL: https://en.wikipedia.org/wiki/World_War_II. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/WWII_(disambiguation). Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/The_Second_World_War_(disambiguation). Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/World_War_II_(disambiguation). Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Junkers_Ju_87. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Eastern_Front_(World_War_II). Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Matilda_II. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/North_African_campaign. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Battle_of_Stalingrad. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Raising_a_Flag_over_the_Reichstag. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Reichstag_building. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Battle_of_Berlin. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Japanese_occupation_of_the_Philippines. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Theater_(warfare). Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/European_theatre_of_World_War_II. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Pacific_War. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Battle_of_the_Atlantic. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Indian_Ocean_in_World_War_II. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/South-East_Asian_theatre_of_World_War_II. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Second_Sino-Japanese_War. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Air_raids_on_Japan. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Mediterranean_and_Middle_East_theatre_of_World_War_II. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Battle_of_Gabon. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Attacks_on_Australia_during_World_War_II. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Battle_of_the_Caribbean. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/American_Theater_(World_War_II). Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Allies_of_World_War_II. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Aftermath_of_World_War_II. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/World_War_II_by_country. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Axis_powers. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Allied_leaders_of_World_War_II. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Soviet_Union. Score: 0.0\n",
      "URL: https://en.wikipedia.org/wiki/Joseph_Stalin. Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "logic_operators = {\"and\", \"or\", \"not\"}\n",
    "\n",
    "def preprocess_query(query: str, exclude_words: set = set()) -> str:\n",
    "\n",
    "    # Init nltk objects\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english')) - exclude_words\n",
    "\n",
    "    # Tokenize query\n",
    "    tokens = word_tokenize(query.lower())\n",
    "    # Remove all the stop words inside the query tokens\n",
    "    non_stopwords_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    # Lematize the remain words\n",
    "    lemmatized_query = \" \".join([lemmatizer.lemmatize(word) for word in non_stopwords_tokens])\n",
    "\n",
    "    return lemmatized_query\n",
    "\n",
    "def ranking_TF_IDF(parsed_scrape: dict, query: str, result_set: set = None):\n",
    "\n",
    "    if not query:\n",
    "        return {}\n",
    "\n",
    "    # Preprocess the query\n",
    "    lemmatized_query = preprocess_query(query, logic_operators)\n",
    "\n",
    "    # Combine the URL and the paragraphs in a signle line\n",
    "    documents = {entry['website_url']: \" \".join(entry['content']) for entry in parsed_scrape}\n",
    "    \n",
    "    # A Result set has been provided\n",
    "    if result_set is not None:\n",
    "\n",
    "        # Remove documents that arent in the result set\n",
    "        documents = {url: content for url, content in documents.items() if url in result_set}\n",
    "\n",
    "    # Init the TF-IDF matrix\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents.values())\n",
    "\n",
    "    query_vector = vectorizer.transform([lemmatized_query])\n",
    "\n",
    "    scores = cosine_similarity(query_vector, tfidf_matrix).flatten()  # Using flatter to conv to vectoer\n",
    "    ranked_results = sorted(zip(documents.keys(), scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked_results\n",
    "    \n",
    "def boolean_retrieval(query: str, index: dict) -> set:\n",
    "    \n",
    "    if not query: \n",
    "        return {}\n",
    "    \n",
    "    # Preprossess query to remove any unwanted words or characters, keeping the logic ops\n",
    "    lemmatized_query = preprocess_query(query, logic_operators)\n",
    "\n",
    "    # Remove any duplicates\n",
    "    result = set()\n",
    "\n",
    "    # Default search op is logic or\n",
    "    op = \"or\"\n",
    "\n",
    "    for token in lemmatized_query.split():\n",
    "        # Chnage mode\n",
    "        if token.lower() in logic_operators:\n",
    "            op = token.lower()\n",
    "            continue\n",
    "\n",
    "        # Token is not found\n",
    "        if token not in index:\n",
    "            continue\n",
    "        \n",
    "        url_list = set(index[token])\n",
    "\n",
    "        # Sets allow logic operations on pyth\n",
    "        if op == \"or\":\n",
    "            result |= url_list  # If or join the two url lists\n",
    "        elif op == \"and\":\n",
    "            result &= url_list  # If and take the common links only\n",
    "        elif op == \"not\":\n",
    "            result -= url_list  # If not remove the links from the result\n",
    "\n",
    "    return result\n",
    "\n",
    "def vsm_retrieval(query: str, parsed_scrape: dict):\n",
    "    if not query:\n",
    "        return {}\n",
    "\n",
    "    processed_query = preprocess_query(query)\n",
    "\n",
    "    results = ranking_TF_IDF(parsed_scrape, processed_query)\n",
    "    return results\n",
    "\n",
    "def probabilistic_retrieval(parsed_scrape: list, query: str):\n",
    "    if not query:\n",
    "        return []\n",
    "\n",
    "    # Preprocess the query\n",
    "    processed_query = preprocess_query(query)\n",
    "\n",
    "    # Combine the URL and the paragraphs in a signle line\n",
    "    documents = {entry['website_url']: \" \".join(entry['content']) for entry in parsed_scrape}\n",
    "\n",
    "    # Tokenize documents\n",
    "    tokenized_documents = [word_tokenize(doc.lower()) for doc in documents.values()]\n",
    "\n",
    "    # Initialize BM25 model\n",
    "    bm25 = BM25Okapi(tokenized_documents)\n",
    "\n",
    "    # Tokenize the query\n",
    "    tokenized_query = word_tokenize(processed_query)\n",
    "\n",
    "    # Get BM25 scores\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    ranked_results = sorted(zip(documents.keys(),scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked_results\n",
    "\n",
    "def dataRetrival(inverted_index: dict, parsed_scrape: dict, option: str, query: str) -> set:\n",
    "    result_set = set()\n",
    "    if option == \"1\":\n",
    "        bool_results = boolean_retrieval(query, inverted_index)\n",
    "        result_set = ranking_TF_IDF(parsed_scrape, query, bool_results)\n",
    "    elif option == \"2\":\n",
    "        result_set = vsm_retrieval(query, parsed_scrape)\n",
    "    elif option == \"3\":\n",
    "        result_set = probabilistic_retrieval(parsed_scrape,query)\n",
    "    else:\n",
    "        raise Exception(f\"Invalid Retrival Method: \\\"{option}\\\"\")\n",
    "    \n",
    "    return result_set\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Open the inverted index save file\n",
    "    with open('inverted_index.json', 'r') as file:\n",
    "        inverted_index = json.load(file)\n",
    "\n",
    "    # Open the parsed data (Used for TF-IDF matrix init)\n",
    "    with open('parsed_scrape.json', 'r') as file:\n",
    "        parsed_scrape = json.load(file)\n",
    "\n",
    "    print(\"Options:\")\n",
    "    print(\"0. Exit\")\n",
    "    print(\"1. Boolean Retrieval\")\n",
    "    print(\"2. Vector Space Model (TF-IDF Ranking)\")\n",
    "    print(\"3. Okapi BM25\")\n",
    "    option = input(\"0,1,2,3: \")\n",
    "    if option == \"0\":\n",
    "        exit()\n",
    "\n",
    "    query = input(\"Request query: \")\n",
    "\n",
    "    result_set = dataRetrival(inverted_index, parsed_scrape, option, query)\n",
    "\n",
    "    print(\"\\nResults: \")\n",
    "    for score, url in result_set:\n",
    "        print(f\"URL: {score}. Score: {url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Βήμα 5. Αξιολόγηση συστήματος:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ground_truths.json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"query\": \"Junkers Ju 87\",\n",
    "        \"links\": [\n",
    "            \"https://en.wikipedia.org/wiki/Junkers_Ju_87\",\n",
    "            \"https://en.wikipedia.org/wiki/Battle_of_Britain\",\n",
    "            \"https://en.wikipedia.org/wiki/Battle_of_France\",\n",
    "            \"https://en.wikipedia.org/wiki/Invasion_of_Poland\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Major Battles\",\n",
    "        \"links\": [\n",
    "            \"https://en.wikipedia.org/wiki/Battle_of_Stalingrad\",\n",
    "            \"https://en.wikipedia.org/wiki/Allies_of_World_War_II\",\n",
    "            \"https://en.wikipedia.org/wiki/Second_Sino-Japanese_War\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"United Kingdom and not Germany\",\n",
    "        \"links\": [\n",
    "            \"https://en.wikipedia.org/wiki/South-East_Asian_theatre_of_World_War_II\",\n",
    "            \"https://en.wikipedia.org/wiki/Japanese_occupation_of_the_Philippines\",\n",
    "            \"https://en.wikipedia.org/wiki/Attacks_on_Australia_during_World_War_II\",\n",
    "            \"https://en.wikipedia.org/wiki/Matilda_II\",\n",
    "            \"https://en.wikipedia.org/wiki/Invasion_of_Lingayen_Gulf\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"United States in the Pacific\",\n",
    "        \"links\": [\n",
    "            \"https://en.wikipedia.org/wiki/United_States\",\n",
    "            \"https://en.wikipedia.org/wiki/Allies_of_World_War_II\",\n",
    "            \"https://en.wikipedia.org/wiki/World_War_II\",\n",
    "            \"https://en.wikipedia.org/wiki/Aftermath_of_World_War_II\",\n",
    "            \"https://en.wikipedia.org/wiki/Axis_powers\",\n",
    "            \"https://en.wikipedia.org/wiki/Soviet_Union\",\n",
    "            \"https://en.wikipedia.org/wiki/South-East_Asian_theatre_of_World_War_II\",\n",
    "            \"https://en.wikipedia.org/wiki/Pacific_War\",\n",
    "            \"https://en.wikipedia.org/wiki/American_theater_(World_War_II)\",\n",
    "            \"https://en.wikipedia.org/wiki/Empire_of_Japan\",\n",
    "            \"https://en.wikipedia.org/wiki/Japanese_occupation_of_the_Philippines\"\n",
    "        ]\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Evaluation Algorithm\n",
      "0. Exit\n",
      "1. Boolean Retrieval\n",
      "2. Vector Space Model (TF-IDF Ranking)\n",
      "3. Okapi BM25\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 74\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid option: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moption\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, question \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ground_truths):\n\u001b[1;32m---> 74\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[43mquestion\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     75\u001b[0m     ground_truths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(question[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinks\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;66;03m# Call the data retrival func from the previuse question\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m#result_set = search_engine_2.dataRetrival(inverted_index, parsed_scrape, option, query)    # Use this outside Jupyter notebook\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "#import search_engine_2     # Use this outside Jupyter nodebook\n",
    "import json\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# Data transfer class for the metrics\n",
    "class EvaluationValues:\n",
    "    _Precision: float\n",
    "    _Recall: float\n",
    "    _F1_score: float\n",
    "    _Map: float\n",
    "\n",
    "    # Print method\n",
    "    def print_values(self):\n",
    "        print(f\"Precission: {self._Precision}\")\n",
    "        print(f\"Recall: {self._Recall}\")\n",
    "        print(f\"F1 Score: {self._F1_score}\")\n",
    "        print(f\"Map: {self._Map}\")\n",
    "\n",
    "\n",
    "def evaluate_query(results, ground_truth_set: set) -> EvaluationValues:\n",
    "    relative_results = []\n",
    "\n",
    "    # Keep only reletive docs\n",
    "    for link, score in results:\n",
    "        if score != 0:\n",
    "            relative_results.append(link)\n",
    "\n",
    "    # Convert to tables for the sklearn lib\n",
    "    y_true = [1 if link in ground_truth_set else 0 for link in relative_results]    \n",
    "    y_pred = [1 if link in relative_results else 0 for link in ground_truth_set]\n",
    "\n",
    "    # Pad with 0 \n",
    "    while len(y_pred) != len(y_true):\n",
    "        y_pred.append(0)\n",
    "\n",
    "    results = EvaluationValues()\n",
    "    results._Precision = metrics.precision_score(y_true, y_pred)\n",
    "    results._Recall = metrics.recall_score(y_true, y_pred)\n",
    "    results._F1_score = metrics.f1_score(y_true, y_pred)\n",
    "    results._Map = metrics.average_precision_score(y_true, y_pred)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Open the inverted index save file\n",
    "    with open('inverted_index.json', 'r') as file:\n",
    "        inverted_index = json.load(file)\n",
    "\n",
    "    # Open the parsed data (Used for TF-IDF matrix init)\n",
    "    with open('parsed_scrape.json', 'r') as file:\n",
    "        parsed_scrape = json.load(file)\n",
    "    \n",
    "    # Use this outside Jupyter notebook\n",
    "    #with open('ground_truths.json', 'r') as file:\n",
    "    #   ground_truths = json.load(file)\n",
    "    #ground_truths = json.dumps(data, indent=4)\n",
    "\n",
    "    # Select the algorithm\n",
    "    print(\"Select Evaluation Algorithm\")\n",
    "    print(\"0. Exit\")\n",
    "    print(\"1. Boolean Retrieval\")\n",
    "    print(\"2. Vector Space Model (TF-IDF Ranking)\")\n",
    "    print(\"3. Okapi BM25\")\n",
    "    \n",
    "    option = input(\"0,1,2,3: \")\n",
    "    \n",
    "    if option == \"0\":\n",
    "        exit()\n",
    "    elif option != \"1\" and option != \"2\" and option != \"3\":\n",
    "        raise Exception(f\"Invalid option: {option}\")\n",
    "\n",
    "    for index, question in enumerate(data):\n",
    "        query = question['query']\n",
    "        ground_truths = set(question['links'])\n",
    "\n",
    "        # Call the data retrival func from the previuse question\n",
    "        #result_set = search_engine_2.dataRetrival(inverted_index, parsed_scrape, option, query)    # Use this outside Jupyter notebook\n",
    "        result_set = dataRetrival(inverted_index, parsed_scrape, option, query)     # Use this inside Jupyter notebook\n",
    "\n",
    "        ev = evaluate_query(result_set, ground_truths)\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"\\nQuery: {query}. Evaluation Values:\")\n",
    "        ev.print_values()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
